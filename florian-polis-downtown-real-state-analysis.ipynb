{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6876358,"sourceType":"datasetVersion","datasetId":3951282}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Florianópolis Downtown Real State Data Analysis**\n\nHello! I'm Eduardo Adriani Rapanos and today I will try to propose many insights about the Florianópolis Downtown Real State Data. \n\nThe data was provided by the LOCATES technologic company, that I work nowadays, with the purpose of I apply the knowledge of the Data Science Course, that I've been doing.\n\nFor maintain the value of the products of the LOCATES co., I selected old adverts from the first semester of 2022.\n\nWe will explore the Real State Data from the Downtown neighborhood of Florianópolis City, Brazil. The data were collected from internet adverts that is, they are real ads. We are interested in the propoerties which are saling, to indentify the **spatial distribution of the apartament meter square of these properties on the neighbohood** and compare secondarily it if a role data, including data without coordinates.\n\nThis processes were developed in this project:\n\n1. Download the data from LOCATES co. database;\n2. Importing the data set to Kaggle;\n3. Data structure and content analysis;\n4. Data cleaning;\n5. Data visualization: analysis between de data with values to lat/lon columns and data with NaN values to these columns;\n6. Data visualization: a Square Meter Apartament Value Map from Downtown of Florianópolis, Santa Catarina, Brazil;\n7. Final Considerations.","metadata":{}},{"cell_type":"markdown","source":"# Importing the data","metadata":{}},{"cell_type":"code","source":"# Lets read the data using a pandas dataframe:\n\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.209591Z","iopub.execute_input":"2023-12-01T10:31:17.209965Z","iopub.status.idle":"2023-12-01T10:31:17.554879Z","shell.execute_reply.started":"2023-12-01T10:31:17.209937Z","shell.execute_reply":"2023-12-01T10:31:17.553485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we are opening our data set and including the 'na' to empty cels:\n\ndf = pd.read_csv('/kaggle/input/florianpolis-downtown-real-state-analysis-fdrsa/tb_mercadoimob.csv', na_values = 'na')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.557193Z","iopub.execute_input":"2023-12-01T10:31:17.557741Z","iopub.status.idle":"2023-12-01T10:31:17.927288Z","shell.execute_reply.started":"2023-12-01T10:31:17.557706Z","shell.execute_reply":"2023-12-01T10:31:17.925684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we will use the .head() method to read the 10 first lines of the archive:\n\ndf.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.928881Z","iopub.execute_input":"2023-12-01T10:31:17.929207Z","iopub.status.idle":"2023-12-01T10:31:17.972723Z","shell.execute_reply.started":"2023-12-01T10:31:17.929181Z","shell.execute_reply":"2023-12-01T10:31:17.970779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will print all the columns of the df to discover which that having information about sale\n\nprint(df.columns)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.975952Z","iopub.execute_input":"2023-12-01T10:31:17.976346Z","iopub.status.idle":"2023-12-01T10:31:17.985001Z","shell.execute_reply.started":"2023-12-01T10:31:17.976311Z","shell.execute_reply":"2023-12-01T10:31:17.983031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The column with the information is 'tipo_negocio'. Lets print the first 10 rows of this column:\n\nprint(df['tipo_negocio'].head(10))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.986354Z","iopub.execute_input":"2023-12-01T10:31:17.986850Z","iopub.status.idle":"2023-12-01T10:31:17.998001Z","shell.execute_reply.started":"2023-12-01T10:31:17.986811Z","shell.execute_reply":"2023-12-01T10:31:17.996211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's true! The **default** columns is 'tipo_negocio'.","metadata":{}},{"cell_type":"code","source":"# Let's print only the **unique** values of the 'tipo_negocio' column:\n\nunique_values = df['tipo_negocio'].unique()\n\nprint(unique_values)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:17.999633Z","iopub.execute_input":"2023-12-01T10:31:18.000059Z","iopub.status.idle":"2023-12-01T10:31:18.016184Z","shell.execute_reply.started":"2023-12-01T10:31:18.000025Z","shell.execute_reply":"2023-12-01T10:31:18.014472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we've only the **'Venda'** and **'Aluguel'** values for this column.","metadata":{}},{"cell_type":"markdown","source":"# **Lets start the structure analysis of the data set:**","metadata":{}},{"cell_type":"code","source":"# We will use the .shape to return the number of lines and columns:\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.017533Z","iopub.execute_input":"2023-12-01T10:31:18.017941Z","iopub.status.idle":"2023-12-01T10:31:18.027320Z","shell.execute_reply.started":"2023-12-01T10:31:18.017909Z","shell.execute_reply":"2023-12-01T10:31:18.025964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we are print the number of the lines whitch the 'Venda' and 'Aluguel' values:\n\ndf[df['tipo_negocio'] == 'Venda'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.029089Z","iopub.execute_input":"2023-12-01T10:31:18.029397Z","iopub.status.idle":"2023-12-01T10:31:18.046372Z","shell.execute_reply.started":"2023-12-01T10:31:18.029375Z","shell.execute_reply":"2023-12-01T10:31:18.044651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['tipo_negocio'] == 'Aluguel'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.048097Z","iopub.execute_input":"2023-12-01T10:31:18.048796Z","iopub.status.idle":"2023-12-01T10:31:18.057898Z","shell.execute_reply.started":"2023-12-01T10:31:18.048758Z","shell.execute_reply":"2023-12-01T10:31:18.056516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do a percentage analysis.\n\n# For this, we are use a placeholder for a variable that we won't use. In this case is the columns number.\n\namt_total, _ = df.shape\namt_venda, _ = df[df['tipo_negocio'] == 'Venda'].shape\namt_aluguel, _ = df[df['tipo_negocio'] == 'Aluguel'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.062159Z","iopub.execute_input":"2023-12-01T10:31:18.063599Z","iopub.status.idle":"2023-12-01T10:31:18.077424Z","shell.execute_reply.started":"2023-12-01T10:31:18.063528Z","shell.execute_reply":"2023-12-01T10:31:18.075836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'The rate of \"Venda\" is {round(100 * amt_venda / amt_total, 2)}%')\n\nprint(f'The rate of \"Aluguel\" is {round(100 * amt_aluguel / amt_total, 2)}%')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.078989Z","iopub.execute_input":"2023-12-01T10:31:18.079456Z","iopub.status.idle":"2023-12-01T10:31:18.085511Z","shell.execute_reply.started":"2023-12-01T10:31:18.079423Z","shell.execute_reply":"2023-12-01T10:31:18.083996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Schema and data processing transformation\n\nNow we will develop many steps to transform our data on way to do a spatial analysis. We will do many others transformations with the objective to analyse our data and to maintain your accuracy.","metadata":{}},{"cell_type":"code","source":"df.head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.087434Z","iopub.execute_input":"2023-12-01T10:31:18.088158Z","iopub.status.idle":"2023-12-01T10:31:18.112651Z","shell.execute_reply.started":"2023-12-01T10:31:18.088124Z","shell.execute_reply":"2023-12-01T10:31:18.111577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use the .dtypes to identify the data type of the cells\n\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.113961Z","iopub.execute_input":"2023-12-01T10:31:18.114548Z","iopub.status.idle":"2023-12-01T10:31:18.127126Z","shell.execute_reply.started":"2023-12-01T10:31:18.114513Z","shell.execute_reply":"2023-12-01T10:31:18.125678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The columns of the dataset are consistent.**","metadata":{}},{"cell_type":"code","source":"# * Category **atributes**: \n\n# selecting the columns of type 'object' and using the describe() method to summarize data.\n# the transpose() method improve the data visualization\n\ndf.select_dtypes('object').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.129002Z","iopub.execute_input":"2023-12-01T10:31:18.129814Z","iopub.status.idle":"2023-12-01T10:31:18.202832Z","shell.execute_reply.started":"2023-12-01T10:31:18.129780Z","shell.execute_reply":"2023-12-01T10:31:18.201372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# * Category **numerics**: \n\ndf.drop('id', axis=1).select_dtypes('number').describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.207105Z","iopub.execute_input":"2023-12-01T10:31:18.207457Z","iopub.status.idle":"2023-12-01T10:31:18.272836Z","shell.execute_reply.started":"2023-12-01T10:31:18.207430Z","shell.execute_reply":"2023-12-01T10:31:18.271028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **No Data (NaN) Analysis**","metadata":{}},{"cell_type":"code","source":"# Let's print all the columns if Nan values. \n\n# True = there is almost one Nan value.\n\n# printing all the columns if NaN values\n\ndf.isna().any()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.275666Z","iopub.execute_input":"2023-12-01T10:31:18.276641Z","iopub.status.idle":"2023-12-01T10:31:18.292538Z","shell.execute_reply.started":"2023-12-01T10:31:18.276590Z","shell.execute_reply":"2023-12-01T10:31:18.291490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nOur objective is spatialize the square meter value of our ads. For this, we will do the next steps:\n","metadata":{}},{"cell_type":"code","source":"# Let's create a function to calcule the % of NaN data in each number column:\n\ndef stats_nan_data(df: pd.DataFrame) -> None:\n    \n    stats_nan_data = []\n    for col in df.columns:\n        if df[col].isna().any():\n            amt, _ = df[df[col].isna()].shape\n            total, _ = df.shape\n            dict_nan_data = {col: {'amount': amt, 'percentage': round(100 * amt/total, 2)}}\n            stats_nan_data.append(dict_nan_data)\n    \n    for stat in stats_nan_data:\n        print(stat)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.294055Z","iopub.execute_input":"2023-12-01T10:31:18.294444Z","iopub.status.idle":"2023-12-01T10:31:18.306199Z","shell.execute_reply.started":"2023-12-01T10:31:18.294353Z","shell.execute_reply":"2023-12-01T10:31:18.305024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print a % of NaN values\n\nstats_nan_data(df=df)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.308059Z","iopub.execute_input":"2023-12-01T10:31:18.308451Z","iopub.status.idle":"2023-12-01T10:31:18.365087Z","shell.execute_reply.started":"2023-12-01T10:31:18.308384Z","shell.execute_reply":"2023-12-01T10:31:18.363849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print a % of NaN values where 'tipo_negocio' == 'Venda'\n\nstats_nan_data(df=df[df['tipo_negocio'] == 'Venda'])","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.366939Z","iopub.execute_input":"2023-12-01T10:31:18.367319Z","iopub.status.idle":"2023-12-01T10:31:18.419676Z","shell.execute_reply.started":"2023-12-01T10:31:18.367286Z","shell.execute_reply":"2023-12-01T10:31:18.417707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print a % of NaN values where 'tipo_negocio' == 'Aluguel'\n\nstats_nan_data(df=df[df['tipo_negocio'] == 'Aluguel'])","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.421272Z","iopub.execute_input":"2023-12-01T10:31:18.421550Z","iopub.status.idle":"2023-12-01T10:31:18.457653Z","shell.execute_reply.started":"2023-12-01T10:31:18.421525Z","shell.execute_reply":"2023-12-01T10:31:18.456394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a high values of percentage with **NaN** value for lat and lon columns. It represents more than 50% of the out data set. Maybe, there aren't information for these columns, for some factors: a lack of information in a complementary columns (numero, logradouro, complemento, etc) or for other reason, these address weren't geocodated.","metadata":{}},{"cell_type":"markdown","source":"# **Choosing and separating NaN at lat and lon columns:**\n\nNow we are identify the **lat** and **lon** columns with NaN values and save in a output table.\n\nWe pretende, on the next steps, compare the data with coordinates and the data without coordinates,\nmainly the sale value of the apartments.","metadata":{}},{"cell_type":"code","source":"# Identifying the NaN rows in lat and lon columns:\n\nnan_rows_df = df[df['lat'].isna() | df['lon'].isna()]","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.458664Z","iopub.execute_input":"2023-12-01T10:31:18.458973Z","iopub.status.idle":"2023-12-01T10:31:18.468215Z","shell.execute_reply.started":"2023-12-01T10:31:18.458948Z","shell.execute_reply":"2023-12-01T10:31:18.467106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exporting the NaN table to output: \n\nnan_rows_df.to_csv('/kaggle/working/nan_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.469322Z","iopub.execute_input":"2023-12-01T10:31:18.469642Z","iopub.status.idle":"2023-12-01T10:31:18.619404Z","shell.execute_reply.started":"2023-12-01T10:31:18.469580Z","shell.execute_reply":"2023-12-01T10:31:18.618406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we are opening our NaN data set and including the 'na' to empty cels:\n\nds = pd.read_csv('/kaggle/working/nan_data.csv', na_values = 'na')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.620530Z","iopub.execute_input":"2023-12-01T10:31:18.620856Z","iopub.status.idle":"2023-12-01T10:31:18.704936Z","shell.execute_reply.started":"2023-12-01T10:31:18.620829Z","shell.execute_reply":"2023-12-01T10:31:18.703171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the lines x columns with our nan_data.csv table:\n\nds.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.707175Z","iopub.execute_input":"2023-12-01T10:31:18.707535Z","iopub.status.idle":"2023-12-01T10:31:18.715366Z","shell.execute_reply.started":"2023-12-01T10:31:18.707507Z","shell.execute_reply":"2023-12-01T10:31:18.713834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the lines x columns with our fully data table:\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.717886Z","iopub.execute_input":"2023-12-01T10:31:18.718221Z","iopub.status.idle":"2023-12-01T10:31:18.726385Z","shell.execute_reply.started":"2023-12-01T10:31:18.718191Z","shell.execute_reply":"2023-12-01T10:31:18.724569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping data from df where lat, lon are NaN:\n\ndf = df.dropna(subset=['lat', 'lon'])","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.728672Z","iopub.execute_input":"2023-12-01T10:31:18.728996Z","iopub.status.idle":"2023-12-01T10:31:18.739269Z","shell.execute_reply.started":"2023-12-01T10:31:18.728968Z","shell.execute_reply":"2023-12-01T10:31:18.737759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the number of rows and columns withou NaN values at lat lon:\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.740462Z","iopub.execute_input":"2023-12-01T10:31:18.740817Z","iopub.status.idle":"2023-12-01T10:31:18.749135Z","shell.execute_reply.started":"2023-12-01T10:31:18.740789Z","shell.execute_reply":"2023-12-01T10:31:18.747345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the number of rows with 'tipo_negocio' = 'Venda':\n\ndf[df['tipo_negocio'] == 'Venda'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.755004Z","iopub.execute_input":"2023-12-01T10:31:18.755394Z","iopub.status.idle":"2023-12-01T10:31:18.767924Z","shell.execute_reply.started":"2023-12-01T10:31:18.755364Z","shell.execute_reply":"2023-12-01T10:31:18.765711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the number of rows with 'tipo_negocio' = 'Aluguel':\n\ndf[df['tipo_negocio'] == 'Aluguel'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.769175Z","iopub.execute_input":"2023-12-01T10:31:18.769490Z","iopub.status.idle":"2023-12-01T10:31:18.781512Z","shell.execute_reply.started":"2023-12-01T10:31:18.769457Z","shell.execute_reply":"2023-12-01T10:31:18.779771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating variables to do some statistics:\n\namt_total_new, _ = df.shape\namt_venda_new, _ = df[df['tipo_negocio'] == 'Venda'].shape\namt_aluguel_new, _ = df[df['tipo_negocio'] == 'Aluguel'].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.783658Z","iopub.execute_input":"2023-12-01T10:31:18.786603Z","iopub.status.idle":"2023-12-01T10:31:18.797186Z","shell.execute_reply.started":"2023-12-01T10:31:18.786572Z","shell.execute_reply":"2023-12-01T10:31:18.795559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'The rate of \"Venda\" is {round(100 * amt_venda / amt_total, 2)}%')\nprint(f'The new rate of \"Venda\" is {round(100 * amt_venda_new / amt_total_new, 2)}%')\nprint('\\n')\nprint(f'The rate of \"Aluguel\" is {round(100 * amt_aluguel / amt_total, 2)}%')\nprint(f'The new rate of \"Aluguel\" is {round(100 * amt_aluguel_new / amt_total_new, 2)}%')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.798641Z","iopub.execute_input":"2023-12-01T10:31:18.798972Z","iopub.status.idle":"2023-12-01T10:31:18.808247Z","shell.execute_reply.started":"2023-12-01T10:31:18.798941Z","shell.execute_reply":"2023-12-01T10:31:18.806735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The comparison of de fully data table with de new table - without NaN's - is above.**\n\nIndeed, we've a +- 1.9% of increase in a 'new rate Aluguel'.\n\nHowever the final value is very near to the original condition.","metadata":{}},{"cell_type":"markdown","source":"**Here, we will create 4 new df to represent the data.**\n\nOne is with 'lat' and 'lon' information and 'tipo_negocio' = 'venda' for our spatial analysis.\n\n\nThe other three are for we do other analysis and verify the accuracy and vigor of our data.","metadata":{}},{"cell_type":"code","source":"# Creating a variable that select only the 'Venda' at the 'tipo_negocio' colum:\n\ndf_venda = df[df['tipo_negocio'] == 'Venda']","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.809799Z","iopub.execute_input":"2023-12-01T10:31:18.810082Z","iopub.status.idle":"2023-12-01T10:31:18.822189Z","shell.execute_reply.started":"2023-12-01T10:31:18.810056Z","shell.execute_reply":"2023-12-01T10:31:18.821172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a variable that select only the 'Aluguel' at the 'tipo_negocio' colum:\n\ndf_aluguel = df[df['tipo_negocio'] == 'Aluguel']","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.823725Z","iopub.execute_input":"2023-12-01T10:31:18.824056Z","iopub.status.idle":"2023-12-01T10:31:18.833021Z","shell.execute_reply.started":"2023-12-01T10:31:18.824029Z","shell.execute_reply":"2023-12-01T10:31:18.831524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ds = the df with NaN values at lat lon columns.\n\n# Creating a variable that select only the 'Venda' at the 'tipo_negocio' colum:\n\nds_venda = ds[ds['tipo_negocio'] == 'Venda']","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.834448Z","iopub.execute_input":"2023-12-01T10:31:18.834784Z","iopub.status.idle":"2023-12-01T10:31:18.846955Z","shell.execute_reply.started":"2023-12-01T10:31:18.834745Z","shell.execute_reply":"2023-12-01T10:31:18.844930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ds = the df with NaN values at lat lon columns.\n\n# Creating a variable that select only the 'Aluguel' at the 'tipo_negocio' colum:\n\nds_aluguel = df[df['tipo_negocio'] == 'Aluguel']","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:18.848938Z","iopub.execute_input":"2023-12-01T10:31:18.850241Z","iopub.status.idle":"2023-12-01T10:31:18.859809Z","shell.execute_reply.started":"2023-12-01T10:31:18.850196Z","shell.execute_reply":"2023-12-01T10:31:18.857697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_venda.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.530943Z","iopub.execute_input":"2023-12-01T10:31:19.531313Z","iopub.status.idle":"2023-12-01T10:31:19.538985Z","shell.execute_reply.started":"2023-12-01T10:31:19.531283Z","shell.execute_reply":"2023-12-01T10:31:19.537002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_venda.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.541383Z","iopub.execute_input":"2023-12-01T10:31:19.541773Z","iopub.status.idle":"2023-12-01T10:31:19.554568Z","shell.execute_reply.started":"2023-12-01T10:31:19.541741Z","shell.execute_reply":"2023-12-01T10:31:19.553023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Remembering our 'object' options:\n\ndf_venda.select_dtypes('object').head(n=5).transpose()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.556118Z","iopub.execute_input":"2023-12-01T10:31:19.556440Z","iopub.status.idle":"2023-12-01T10:31:19.574976Z","shell.execute_reply.started":"2023-12-01T10:31:19.556411Z","shell.execute_reply":"2023-12-01T10:31:19.573538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print the columns of our datasets:\n\nprint(df_venda.columns)\nprint(ds_venda.columns)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.577885Z","iopub.execute_input":"2023-12-01T10:31:19.578369Z","iopub.status.idle":"2023-12-01T10:31:19.587041Z","shell.execute_reply.started":"2023-12-01T10:31:19.578331Z","shell.execute_reply":"2023-12-01T10:31:19.585715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For the purpose of calculate the square meter value**\n\nWe will create a columns 'valor_m2' at the df_venda e ds_venda.","metadata":{}},{"cell_type":"code","source":"df_venda['valor_m2'] = df_venda['valor'] / df_venda['area']\n\nds_venda['valor_m2'] = ds_venda['valor'] / ds_venda['area']","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.592436Z","iopub.execute_input":"2023-12-01T10:31:19.593765Z","iopub.status.idle":"2023-12-01T10:31:19.607532Z","shell.execute_reply.started":"2023-12-01T10:31:19.593725Z","shell.execute_reply":"2023-12-01T10:31:19.606330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It's done!\n","metadata":{}},{"cell_type":"markdown","source":"# **Data Visualization**\n\nThe data were done. Now, we will create many visualizations to correlact the explicative variables with the anser variable ('tipo_negocio' = 'Venda') to understad the spatial distribution of the meter square on the Downtown of Florianópolis.","metadata":{}},{"cell_type":"code","source":"# Importing new libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport numpy as np\n\nfrom folium import plugins\n\nfrom shapely.geometry import Point, Polygon\nfrom folium.plugins import HeatMap\n\n \n# Selecting the general style with witegrid\n\nsns.set_style('whitegrid')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:19.609651Z","iopub.execute_input":"2023-12-01T10:31:19.610337Z","iopub.status.idle":"2023-12-01T10:31:22.043098Z","shell.execute_reply.started":"2023-12-01T10:31:19.610305Z","shell.execute_reply":"2023-12-01T10:31:22.041829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will combine the columns 'tipo_imovel' with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'tipo_imovel'\ntitles = ['Type of property at non NaN df', 'Type of property at NaN df']\n\nmax_y = 0\nmax_val = df[column].value_counts().max() * 1.1 \n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df, ds]):\n    \n    df_to_plot = dataframe[column].value_counts().reset_index()\n    df_to_plot.columns = ['type', 'absolute_frequency']\n    df_to_plot.sort_values(by='type', inplace=True)\n    \n    f = sns.barplot(x='type', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel=column.replace('_', ' ').capitalize(), ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_val))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:22.044375Z","iopub.execute_input":"2023-12-01T10:31:22.045849Z","iopub.status.idle":"2023-12-01T10:31:22.755148Z","shell.execute_reply.started":"2023-12-01T10:31:22.045787Z","shell.execute_reply":"2023-12-01T10:31:22.754415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparing the df and ds** we can see that ds, indeed, there are more records at any classes at 'tipo_imovel'.","metadata":{}},{"cell_type":"code","source":"# Here we will combine the columns 'tipo_imovel' == 'Venda' with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'tipo_imovel'\ntitles = ['Type of property from \"Venda\" non NaN df_venda', 'Type of property from \"Venda\" NaN ds_venda']\n\nmax_y = 0\nmax_val = df[column].value_counts().max() * 1.1 \n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df_venda, ds_venda]):\n    \n    df_to_plot = dataframe[column].value_counts().reset_index()\n    df_to_plot.columns = ['type', 'absolute_frequency']\n    df_to_plot.sort_values(by='type', inplace=True)\n    \n    f = sns.barplot(x='type', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel=column.replace('_', ' ').capitalize(), ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_val))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:22.757114Z","iopub.execute_input":"2023-12-01T10:31:22.757436Z","iopub.status.idle":"2023-12-01T10:31:23.558815Z","shell.execute_reply.started":"2023-12-01T10:31:22.757407Z","shell.execute_reply":"2023-12-01T10:31:23.557678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Here we will combine the columns 'tipo_imovel' == 'Aluguel' with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'tipo_imovel'\ntitles = ['Type of property from \"Aluguel\" non NaN df_aluguel', 'Type of property from \"Aluguel\" NaN ds_aluguel']\n\nmax_y = 0\nmax_val = df[column].value_counts().max() * 1.1 \n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df_aluguel, ds_aluguel]):\n    \n    df_to_plot = dataframe[column].value_counts().reset_index()\n    df_to_plot.columns = ['type', 'absolute_frequency']\n    df_to_plot.sort_values(by='type', inplace=True)\n    \n    f = sns.barplot(x='type', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel=column.replace('_', ' ').capitalize(), ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_val))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:23.560942Z","iopub.execute_input":"2023-12-01T10:31:23.561332Z","iopub.status.idle":"2023-12-01T10:31:24.134432Z","shell.execute_reply.started":"2023-12-01T10:31:23.561299Z","shell.execute_reply":"2023-12-01T10:31:24.133155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we are visualize the relation between the default and numeric atributes:\n\ndf_venda.drop(['date_part', 'id', 'estado_construcao', 'bl_temporada', 'cep', 'numero', 'precisao', 'is_outlier'], axis = 1).select_dtypes('number').head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:24.135803Z","iopub.execute_input":"2023-12-01T10:31:24.136129Z","iopub.status.idle":"2023-12-01T10:31:24.160004Z","shell.execute_reply.started":"2023-12-01T10:31:24.136094Z","shell.execute_reply":"2023-12-01T10:31:24.158492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we are visualize the relation between the default and numeric atributes:\n\nds_venda.drop(['date_part', 'id', 'estado_construcao', 'bl_temporada', 'cep', 'numero', 'precisao', 'is_outlier'], axis = 1).select_dtypes('number').head(n=5)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:24.161518Z","iopub.execute_input":"2023-12-01T10:31:24.161874Z","iopub.status.idle":"2023-12-01T10:31:24.186713Z","shell.execute_reply.started":"2023-12-01T10:31:24.161848Z","shell.execute_reply":"2023-12-01T10:31:24.184311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will combine the columns 'area' clustering in a specific with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'area'\ntitles = ['Area from non NaN df', 'Area from NaN ds']\n\nmax_y = 0\n\n\nbins = [0, 25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, float('inf')]\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df, ds]):\n    # Use pd.cut() para criar intervalos e contar quantos valores estão em cada intervalo\n    df_to_plot = pd.cut(dataframe[column], bins, right=False).value_counts().reset_index()\n    df_to_plot.columns = ['area_interval', 'absolute_frequency']\n    df_to_plot.sort_values(by='area_interval', inplace=True)\n    \n    f = sns.barplot(x='area_interval', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel='Area Interval', ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_y))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:24.188628Z","iopub.execute_input":"2023-12-01T10:31:24.189012Z","iopub.status.idle":"2023-12-01T10:31:24.854749Z","shell.execute_reply.started":"2023-12-01T10:31:24.188982Z","shell.execute_reply":"2023-12-01T10:31:24.852888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Here we will combine the columns 'area' clustering in a specific with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'area'\ntitles = ['Area from \"Venda\" non NaN df_venda', 'Area from \"Venda\" NaN ds_venda']\n\nmax_y = 0\n\n\nbins = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, float('inf')]\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df_venda, ds_venda]):\n    # Use pd.cut() para criar intervalos e contar quantos valores estão em cada intervalo\n    df_to_plot = pd.cut(dataframe[column], bins, right=False).value_counts().reset_index()\n    df_to_plot.columns = ['area_interval', 'absolute_frequency']\n    df_to_plot.sort_values(by='area_interval', inplace=True)\n    \n    f = sns.barplot(x='area_interval', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel='Area Interval', ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_y))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:24.856076Z","iopub.execute_input":"2023-12-01T10:31:24.856539Z","iopub.status.idle":"2023-12-01T10:31:25.481237Z","shell.execute_reply.started":"2023-12-01T10:31:24.856507Z","shell.execute_reply":"2023-12-01T10:31:25.479969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will combine the columns 'valor' clustering in a specific with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'valor'\ntitles = ['Property Value from non NaN df', 'Property Value from NaN ds']\n\nmax_y = 0\n\n\nbins = [0, 50000, 100000, 150000, 200000, 300000, 400000, 500000, 600000, 700000, 800000, 900000, 1000000, 1100000, 1200000, 1500000, 1700000, 2000000, 2500000, 30000000, float('inf')]\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df, ds]):\n    # Use pd.cut() para criar intervalos e contar quantos valores estão em cada intervalo\n    df_to_plot = pd.cut(dataframe[column], bins, right=False).value_counts().reset_index()\n    df_to_plot.columns = ['property_value', 'absolute_frequency']\n    df_to_plot.sort_values(by='property_value', inplace=True)\n    \n    f = sns.barplot(x='property_value', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel='Property Value', ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_y))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:25.482455Z","iopub.execute_input":"2023-12-01T10:31:25.482723Z","iopub.status.idle":"2023-12-01T10:31:26.326575Z","shell.execute_reply.started":"2023-12-01T10:31:25.482701Z","shell.execute_reply":"2023-12-01T10:31:26.325241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will combine the columns 'valor' clustering in a specific with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'valor'\ntitles = ['Property Value from non NaN df_venda', 'Property Value from NaN ds_venda']\n\nmax_y = 0\n\n\nbins = [0, 50000, 100000, 150000, 200000, 300000, 400000, 500000, 600000, 700000, 800000, 900000, 1000000, 1100000, 1200000, 1500000, 1700000, 2000000, 2500000, 30000000, float('inf')]\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df_venda, ds_venda]):\n    # Use pd.cut() para criar intervalos e contar quantos valores estão em cada intervalo\n    df_to_plot = pd.cut(dataframe[column], bins, right=False).value_counts().reset_index()\n    df_to_plot.columns = ['property_value', 'absolute_frequency']\n    df_to_plot.sort_values(by='property_value', inplace=True)\n    \n    f = sns.barplot(x='property_value', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel='Property Value', ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n    f.set(ylim=(0, max_y))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:26.328090Z","iopub.execute_input":"2023-12-01T10:31:26.328438Z","iopub.status.idle":"2023-12-01T10:31:27.188836Z","shell.execute_reply.started":"2023-12-01T10:31:26.328410Z","shell.execute_reply":"2023-12-01T10:31:27.187125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we will combine the columns 'valor' clustering in a specific with your absolute frequency between the non NaN df (df) and NaN df (ds): \n\ncolumn = 'valor_m2'\ntitles = ['Property Value from non NaN df_venda', 'Property Value from NaN ds_venda']\n\nmax_y = 0\n\n\nbins = [0, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000, 22500, 25000, 30000, float('inf')]\nmax_y = 0\n\nfigure, axis = plt.subplots(1, 2, figsize=(20, 5), sharex=True)\n\nfor axle, dataframe in enumerate([df_venda, ds_venda]):\n    df_to_plot = pd.cut(dataframe[column], bins, right=False).value_counts().reset_index()\n    df_to_plot.columns = ['property_value', 'absolute_frequency']\n    df_to_plot.sort_values(by='property_value', inplace=True)\n    \n    f = sns.barplot(x='property_value', y='absolute_frequency', data=df_to_plot, ax=axis[axle])\n    f.set(title=titles[axle], xlabel='Square meter value', ylabel='Absolute Frequency')\n    f.set_xticklabels(labels=f.get_xticklabels(), rotation=90)\n    _, max_y_f = f.get_ylim()\n    max_y = max_y_f if max_y_f > max_y else max_y\n\n# Defina o mesmo limite superior (max_y) para ambos os gráficos\nfor ax in axis:\n    ax.set(ylim=(0, max_y))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:27.193124Z","iopub.execute_input":"2023-12-01T10:31:27.193439Z","iopub.status.idle":"2023-12-01T10:31:27.824642Z","shell.execute_reply.started":"2023-12-01T10:31:27.193413Z","shell.execute_reply":"2023-12-01T10:31:27.823310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Both of df_venda and ds_venda** have the same pattern at the bars of the graph!","metadata":{}},{"cell_type":"markdown","source":"# Let's create our maps and do our spatial analysis!\n\nAt the end, and for finish with our main objective **to represent the Apartament Square Meter Value in the Downtown of Florianópolis City** we will create some maps to represent it.","metadata":{}},{"cell_type":"code","source":"import folium\nimport ipywidgets as widgets\n\nfrom folium import LayerControl\n\nfrom folium.plugins import HeatMap\nfrom branca.colormap import LinearColormap\nfrom folium.plugins import HeatMap\nfrom branca.colormap import LinearColormap\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:27.826022Z","iopub.execute_input":"2023-12-01T10:31:27.826389Z","iopub.status.idle":"2023-12-01T10:31:27.832394Z","shell.execute_reply.started":"2023-12-01T10:31:27.826355Z","shell.execute_reply":"2023-12-01T10:31:27.831019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Yes we still have problmens with NaNs. Probably the nans are in the 'valor_m2' column\n\ndf_venda_clean = df_venda.dropna(subset=['lat', 'lon', 'valor_m2'])\ndf_venda_clean = df_venda_clean[df_venda_clean['tipo_imovel'] == 'Apartamento']\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:27.834525Z","iopub.execute_input":"2023-12-01T10:31:27.835106Z","iopub.status.idle":"2023-12-01T10:31:27.850780Z","shell.execute_reply.started":"2023-12-01T10:31:27.834984Z","shell.execute_reply":"2023-12-01T10:31:27.848747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a folium map\nm = folium.Map(location=[df_venda_clean['lat'].mean(), df_venda_clean['lon'].mean()], zoom_start=15)\n\n# Adding simple markes for each point \nfor row in df_venda_clean.itertuples():\n    folium.CircleMarker(\n        location=[row.lat, row.lon],\n        radius=5,  \n        color='blue',  \n        fill=True,\n        fill_color='blue',  \n        fill_opacity=0.6,  \n        popup=f'Square meter value: R${row.valor_m2:.2f}'\n    ).add_to(m)\n\n# Show de interactive map\ndisplay(m)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:27.852826Z","iopub.execute_input":"2023-12-01T10:31:27.853193Z","iopub.status.idle":"2023-12-01T10:31:30.800162Z","shell.execute_reply.started":"2023-12-01T10:31:27.853158Z","shell.execute_reply":"2023-12-01T10:31:30.798708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a map using a CartoDB style\nm = folium.Map(location=[df_venda_clean['lat'].mean(), df_venda_clean['lon'].mean()], zoom_start=15, tiles='cartodbpositron')\n\n# Creating a list wifh coordinates and associated values\ndata = [[row['lat'], row['lon'], row['valor_m2']] for index, row in df_venda_clean.iterrows()]\n\n# Setting a collor gradient\ncolormap = LinearColormap(\n    colors = ['#FFFFCC', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#FC4E2A', '#E31A1C', '#BD0026', '#800026'],\n    index=[0, 5000, 10000, 12500, 15000, 20000, 25000],\n    vmin=0,\n    vmax=25000\n)\n\n# Creating a heatmap with the associanted values\nHeatMap(data, gradient={0.0: colormap(0), 0.1667: colormap(5000), 0.3333: colormap(10000), 0.5: colormap(15000), 0.6667: colormap(20000), 0.8333: colormap(25000), 1.0: colormap(30000)}).add_to(m)\n\n# Adding legends at the map\ncolormap.caption = 'Square meter value'\ncolormap.add_to(m)\n\n# Showing the interactive map\nm\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:30.801970Z","iopub.execute_input":"2023-12-01T10:31:30.802275Z","iopub.status.idle":"2023-12-01T10:31:31.004933Z","shell.execute_reply.started":"2023-12-01T10:31:30.802247Z","shell.execute_reply":"2023-12-01T10:31:31.003695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's integrate a point and heatmap!**","metadata":{}},{"cell_type":"code","source":"# Creating a map using a CartoDB style\nm = folium.Map(\n    location=[df_venda_clean['lat'].mean(), df_venda_clean['lon'].mean()],\n    zoom_start=15,\n    tiles='cartodbpositron'\n)\n\n# Creating a list wifh coordinates and associated values\ndata = [[row['lat'], row['lon'], row['valor_m2']] for index, row in df_venda_clean.iterrows()]\n\n# Defining a collor gradient\ncolormap = LinearColormap(\n    colors=['#FFFFCC', '#FFEDA0', '#FED976', '#FEB24C', '#FD8D3C', '#E31A1C', '#BD0026'],\n    index=[0, 5000, 10000, 12500, 15000, 20000, 25000],\n    vmin=0,\n    vmax=25000\n)\n\n\n# Creating a heatmap with the associanted values\nHeatMap(data, gradient={0.1667: colormap(5000), 0.3333: colormap(10000), 0.5: colormap(15000), 0.6667: colormap(20000), 0.8333: colormap(25000), 1.0: colormap(30000)}, name='Heat Map').add_to(m)\n\n# adding legend in the map\ncolormap.caption = 'Square meter Apartaments value'\ncolormap.add_to(m)\n\n# Adding simple markes for each point \npoints_layer = folium.FeatureGroup(name=\"Points\")\nfor row in df_venda_clean.itertuples():\n    folium.CircleMarker(\n        location=[row.lat, row.lon],\n        radius=3.2,  \n        color='rgba(225, 225, 225, 0.35)',  \n        fill=True,\n        fill_color='rgba(135, 135, 135, 0.35)',  \n        fill_opacity=0.6,\n        popup=f'Square meter Apartament value: R${row.valor_m2:.2f}'\n    ).add_to(points_layer)\n\n# Adding layers to map\npoints_layer.add_to(m)\n\n# Addomg a map controls\nLayerControl().add_to(m)\n\n# Showing the interactive map\nm\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:31.006519Z","iopub.execute_input":"2023-12-01T10:31:31.006939Z","iopub.status.idle":"2023-12-01T10:31:34.110326Z","shell.execute_reply.started":"2023-12-01T10:31:31.006907Z","shell.execute_reply":"2023-12-01T10:31:34.108629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the map in HTML\n\nm.save(\"heatmap_fdrsa.html\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T10:31:34.111963Z","iopub.execute_input":"2023-12-01T10:31:34.112262Z","iopub.status.idle":"2023-12-01T10:31:36.838072Z","shell.execute_reply.started":"2023-12-01T10:31:34.112239Z","shell.execute_reply":"2023-12-01T10:31:36.836402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Considerations\n\nAt the final of this data acquisition,cleanig,transforming and visualizing and many skills was acquired. \n\n* One of the most important was the habit to was documenting the process through the codes and text. This turned the work easier because this project was developed more than a single day.\n\n* Other important factor is relationed with the full knowledge about the data and its problemns. This is an important and crucial thing to observe and develope and improve the analytic skills in the next work.\n\n* At the end, I can get the main objective to spacialize the Square Meter of the Apartaments at the Downtown in Florianópolis.\n\nI'm so glad about this work.\n\nIf you have any consideration or apointment, cosider send a message, review or post about it.\n\nCya.","metadata":{}}]}